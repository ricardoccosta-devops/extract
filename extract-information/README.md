# Plataforma de Processamento de Documentos

## ğŸ“‹ DescriÃ§Ã£o

Plataforma inteligente de leitura e processamento de documentos com suporte a mÃºltiplos formatos (PDF, TXT, DOC, PPT), mÃºltiplas fontes (upload, caminhos locais/rede, S3, Azure) e integraÃ§Ã£o com diversos Large Language Models (LLMs).

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.11 ou superior
- pip

### Passos

1. Clone o repositÃ³rio ou navegue atÃ© o diretÃ³rio do projeto

2. Crie um ambiente virtual (recomendado):
```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Configure as variÃ¡veis de ambiente:
```bash
# Copie o arquivo env.example para .env
cp env.example .env
# Ou no Windows:
copy env.example .env
# Edite o arquivo .env com suas chaves de API e configuraÃ§Ãµes
```

## ğŸ¯ Uso

Execute a aplicaÃ§Ã£o Streamlit:

```bash
streamlit run main.py
```

A aplicaÃ§Ã£o serÃ¡ aberta automaticamente no navegador em `http://localhost:8501`.

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ main.py                 # Ponto de entrada da aplicaÃ§Ã£o
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â”œâ”€â”€ .env.example           # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ config/                # ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ modules/               # MÃ³dulos principais
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ file_ingestion.py  # IngestÃ£o de arquivos
â”‚   â”œâ”€â”€ file_processing.py # Processamento e extraÃ§Ã£o
â”‚   â”œâ”€â”€ output_formatter.py # FormataÃ§Ã£o de saÃ­da
â”‚   â”œâ”€â”€ llm_manager.py     # Gerenciador de LLMs
â”‚   â””â”€â”€ prompt_manager.py  # Gerenciador de prompts
â”œâ”€â”€ providers/            # Providers de LLM
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_provider.py
â”‚   â”œâ”€â”€ openai_client.py
â”‚   â”œâ”€â”€ anthropic_client.py
â”‚   â”œâ”€â”€ ollama_client.py
â”‚   â””â”€â”€ bedrock_client.py
â””â”€â”€ frontend/             # Interface Streamlit
    â””â”€â”€ app.py
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Configure as seguintes variÃ¡veis no arquivo `.env`:

- **LLM Providers**: Configure as chaves de API dos providers que deseja usar
- **Cloud Storage**: Configure credenciais para S3 e/ou Azure (opcional)
- **Application Settings**: Ajuste limites e configuraÃ§Ãµes da aplicaÃ§Ã£o

### Formatos Suportados

- **Entrada**: PDF, TXT, DOCX, PPTX
- **SaÃ­da**: JSON, XML, CSV, TXT

### LLM Providers Suportados

- OpenAI (GPT-3.5, GPT-4)
- Anthropic (Claude)
- Ollama (modelos locais)
- AWS Bedrock

## ğŸ§© Funcionalidades

- âœ… Upload mÃºltiplo de arquivos
- âœ… Leitura de caminhos locais e de rede
- âœ… IntegraÃ§Ã£o com S3 e Azure Blob Storage
- âœ… ExtraÃ§Ã£o de texto de mÃºltiplos formatos
- âœ… Processamento com LLMs (opcional)
- âœ… **Prompts personalizados para extraÃ§Ã£o de informaÃ§Ãµes**
- âœ… Prompts prÃ©-definidos para casos de uso comuns
- âœ… ExportaÃ§Ã£o em mÃºltiplos formatos
- âœ… Cache para melhor performance
- âœ… Logging estruturado
- âœ… Interface intuitiva com Streamlit

## ğŸ“ Exemplos de Uso

### Processamento Simples

1. Selecione "Upload" como fonte
2. FaÃ§a upload de um arquivo PDF
3. Escolha o formato de saÃ­da (ex: JSON)
4. Clique em "Processar Arquivos"
5. Baixe o resultado formatado

### Processamento com LLM e Prompts Personalizados

1. Ative a opÃ§Ã£o "Usar LLM para anÃ¡lise"
2. Selecione um provider (ex: OpenAI)
3. Escolha um modelo (ex: gpt-3.5-turbo)
4. **Configure seu prompt:**
   - **Modo PrÃ©-definido**: Escolha entre prompts prÃ©-configurados como:
     - Resumo
     - Extrair InformaÃ§Ãµes Principais
     - AnÃ¡lise de Dados
     - Perguntas e Respostas
     - Estrutura do Documento
     - Extrair Entidades
     - E outros...
   - **Modo Personalizado**: Digite seu prÃ³prio prompt ou pergunta
     - Use `{content}` como placeholder para o conteÃºdo do documento
     - Exemplo: "Extraia todas as datas importantes e eventos mencionados no documento:\n\n{content}"
5. Processe o arquivo normalmente
6. Veja a anÃ¡lise do LLM na aba "Resultados", incluindo o prompt utilizado

## ğŸ”§ Desenvolvimento

### Estrutura Modular

A aplicaÃ§Ã£o segue uma arquitetura modular que permite:

- Adicionar novos formatos de arquivo facilmente
- Integrar novos providers de LLM
- Adicionar novos formatos de saÃ­da
- Modificar mÃ³dulos sem impactar outros

### Adicionar um Novo Provider de LLM

1. Crie um novo arquivo em `providers/` (ex: `new_provider.py`)
2. Implemente a classe herdando de `BaseLLMProvider`
3. Implemente os mÃ©todos `generate_response()` e `get_available_models()`
4. Adicione o provider ao `LLMManager`

## ğŸ“„ LicenÃ§a

Este projeto Ã© fornecido como estÃ¡, para uso interno.

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests.

